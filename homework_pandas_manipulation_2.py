# -*- coding: utf-8 -*-
"""homework_pandas_manipulation_2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16-I0D0gSzh_u6kEQRBEecOqKvD-o8cdo

# Setup environment and load data

Dataset: https://s3.hothienlac.com/yomitoon/sales_data.csv
"""

import pandas as pd

df_data = pd.read_csv('sales_data.csv')

df_data.head()



"""# ðŸŸ¡ LEVEL 4 â€” Analytical Aggregation (Score 4â€“6)

## **Q8. Average order value (AOV) per customer**

### Task

For each customer, compute:

* total spending
* number of orders
* **average order value**

### ðŸ’¡ Hint

Use:

* `.groupby()`
* `.agg()`
* basic arithmetic between aggregated columns

### ðŸ“š Reference

* [https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.groupby.html](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.groupby.html)
* [https://pandas.pydata.org/docs/user_guide/groupby.html](https://pandas.pydata.org/docs/user_guide/groupby.html)

### ðŸ§  Explanation

This teaches:

* multi-metric aggregation
* **ratio metrics** (very common in dashboards)
* separating *raw data* from *business KPIs*
"""

df_data['total_amount'] = df_data['quantity'] * df_data['unit_price']

aov_per_customer = (
    df_data
    .groupby("customer_id")
    .agg(
        total_spending=("total_amount", "sum"),
        number_of_orders=("order_id", "nunique")
    )
)

# TÃ­nh AOV
aov_per_customer["AOV"] = (
    aov_per_customer["total_spending"] /
    aov_per_customer["number_of_orders"]
)

aov_per_customer



"""## **Q9. Revenue contribution by category (%)**

### Task

Calculate:

* total revenue per product category
* percentage contribution of each category to total revenue

### ðŸ’¡ Hint

Use:

* `.groupby()`
* `.sum()`
* `.assign()`
* division by a **global scalar**

### ðŸ“š Reference

* [https://pandas.pydata.org/docs/reference/api/pandas.Series.div.html](https://pandas.pydata.org/docs/reference/api/pandas.Series.div.html)

### ðŸ§  Explanation

You learn:

* **normalization**
* how to compare groups on the same scale
* how to prepare data for **pie charts / stacked bars**
"""

category_revenue = df_data.groupby('category')['total_amount'].sum().reset_index()
category_revenue = category_revenue.rename(columns={'total_amount': 'total_revenue'})

total_overall_revenue = category_revenue['total_revenue'].sum()

category_revenue = category_revenue.assign(
    percentage_contribution = (category_revenue['total_revenue'] / total_overall_revenue) * 100
)

print(category_revenue.sort_values(by='percentage_contribution', ascending=False))





"""# ðŸ”µ LEVEL 5 â€” Distribution, Ranking & Segmentation (Score 7â€“8)

## **Q10. Identify top 20% customers by revenue (Pareto analysis)**

### Task

Determine:

* which customers belong to the **top 20%** by total spending

### ðŸ’¡ Hint

Use:

* `.sort_values()`
* `.cumsum()`
* `.quantile()`

### ðŸ“š Reference

* [https://pandas.pydata.org/docs/reference/api/pandas.Series.quantile.html](https://pandas.pydata.org/docs/reference/api/pandas.Series.quantile.html)

### ðŸ§  Explanation

This is a classic **80/20 rule** problem:

* who really drives revenue?
* foundational for **customer segmentation**
"""

top_20_percent_customers = customers_sorted_by_spending[customers_sorted_by_spending['cumulative_percentage'] <= 20]
print("Top 20% customers by revenue:")
print(top_20_percent_customers[['total_spending', 'cumulative_percentage']])

customers_sorted_by_spending = aov_per_customer.sort_values(by='total_spending', ascending=False)
print(customers_sorted_by_spending.head())

total_revenue = customers_sorted_by_spending['total_spending'].sum()
customers_sorted_by_spending['cumulative_percentage'] = customers_sorted_by_spending['total_spending'].cumsum() / total_revenue * 100
print(customers_sorted_by_spending.head())

spending_threshold = customers_sorted_by_spending['total_spending'].quantile(0.80)
top_20_percent_customers = customers_sorted_by_spending[customers_sorted_by_spending['total_spending'] >= spending_threshold]
print("Top 20% customers by individual total spending (using 80th percentile threshold):")
print(top_20_percent_customers[['total_spending', 'cumulative_percentage']])

"""## **Q11. Price distribution analysis per category**

### Task

For each product category, compute:

* mean unit price
* median unit price
* standard deviation

### ðŸ’¡ Hint

Use:

* `.groupby()`
* `.agg(mean=..., median=..., std=...)`

### ðŸ“š Reference

* [https://pandas.pydata.org/docs/reference/api/pandas.Series.std.html](https://pandas.pydata.org/docs/reference/api/pandas.Series.std.html)

### ðŸ§  Explanation

This builds intuition for:

* **distribution shape**
* why **median â‰  mean**
* choosing the right chart (boxplot vs bar)
"""

price_stats_per_category = (
    df_data
    .groupby("category")
    .agg(
        mean_price=("unit_price", "mean"),
        median_price=("unit_price", "median"),
        std_price=("unit_price", "std")
    )
)

price_stats_per_category



"""# ðŸ”´ LEVEL 6 â€” Time Series, Growth & Insight (Score 9â€“10)

## **Q12. Day-over-day revenue growth (%)**

### Task

Compute:

* daily revenue
* **percentage change compared to previous day**

### ðŸ’¡ Hint

Use:

* `.groupby()`
* `.pct_change()`

### ðŸ“š Reference

* [https://pandas.pydata.org/docs/reference/api/pandas.Series.pct_change.html](https://pandas.pydata.org/docs/reference/api/pandas.Series.pct_change.html)

### ðŸ§  Explanation

This teaches:

* growth vs absolute value
* preparing data for **line charts**
* understanding volatility
"""

daily_revenue = (
    df_data
    .groupby("order_date")["total_amount"]
    .sum()
    .sort_index()
)

daily_revenue

daily_growth_pct = daily_revenue.pct_change() * 100

daily_growth_pct

daily_revenue_df = pd.DataFrame({
    "daily_revenue": daily_revenue,
    "dod_growth_pct": daily_growth_pct
})

daily_revenue_df

"""## **Q13. Rolling average of daily revenue**

### Task

Calculate:

* 3-day rolling average of daily revenue

### ðŸ’¡ Hint

Use:

* `.rolling(window=3)`
* `.mean()`

### ðŸ“š Reference

* [https://pandas.pydata.org/docs/reference/api/pandas.Series.rolling.html](https://pandas.pydata.org/docs/reference/api/pandas.Series.rolling.html)

### ðŸ§  Explanation

Rolling metrics are used to:

* smooth noisy data
* reveal trends
* support **time-series visualization**
"""

rolling_3d_avg = daily_revenue.rolling(window=3).mean()

rolling_3d_avg

daily_revenue_df["revenue_3d_rolling_avg"] = (
    daily_revenue_df["daily_revenue"]
    .rolling(window=3)
    .mean()
)

daily_revenue_df

"""## **Q14. Detect unusually large orders (outliers)**

### Task

Flag orders where:

* `total_amount` is significantly higher than normal
  (use a statistical threshold)

### ðŸ’¡ Hint

Use:

* `.mean()`
* `.std()`
* boolean conditions

### ðŸ“š Reference

* [https://pandas.pydata.org/docs/reference/api/pandas.Series.std.html](https://pandas.pydata.org/docs/reference/api/pandas.Series.std.html)

### ðŸ§  Explanation

You are learning:

* **basic anomaly detection**
* how math supports intuition
* how analysts decide what deserves investigation
"""

mean_amount = df_data["total_amount"].mean()
std_amount = df_data["total_amount"].std()

mean_amount

std_amount

threshold = mean_amount + 3 * std_amount

threshold

# Flag cÃ¡c Ä‘Æ¡n hÃ ng báº¥t thÆ°á»ng
df_data["is_large_outlier"] = df_data["total_amount"] > threshold

# Xem cÃ¡c Ä‘Æ¡n bá»‹ flag
outliers = df_data[df_data["is_large_outlier"]]
outliers

